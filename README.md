* Generate index name (github repo)
* Configuring those datasets which are are indexing (only stds, testing), generate JSON
* Creating and deleting index
* Running code to crawl
* Storing last URL for crawl
  (JSON in /data)
* Process updates and deletes
* Generating report
  Tag usage, etc
